---
title: "\u2318 Omni CLI"
description: AI-powered terminal assistant for developers
---

import { ProductOverview } from "@/components/docs";
import app from "@/lib/config/app.config";
import { PiTerminalFill, PiRobotFill, PiCodeFill } from "react-icons/pi";

<ProductOverview
  tags={[
    {
      label: "CLI",
      icon: <PiTerminalFill />,
      className: "bg-green-100 text-green-800 dark:bg-green-900/20 dark:text-green-300",
    },
    {
      label: "AI Agent",
      icon: <PiRobotFill />,
      className: "bg-purple-100 text-purple-800 dark:bg-purple-900/20 dark:text-purple-300",
    },
    {
      label: "Developer Tool",
      icon: <PiCodeFill />,
      className: "bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300",
    },
  ]}
  links={[
    {
      href: "https://omni.dev/cli",
      label: "Visit Website",
      type: "website",
    },
    {
      href: `${app.socials.github}/cli-stack`,
      label: "Visit Repository",
      type: "repository",
    },
    {
      href: "/grid/beacon",
      label: "See Beacon (normie interface)",
    },
  ]}
  alerts={[
    {
      title: "ðŸš§ Under Construction",
      description:
        "Omni CLI is in active development. Expect this documentation to change rapidly.",
    },
    {
      title: "ðŸ’¡ Big Idea",
      description:
        "A powerful AI agent in your terminal that understands your codebase, executes tools, and learns from your preferences",
    },
    {
      title: "ðŸš© Problem Solved",
      description:
        "Omni CLI gives developers an AI assistant that lives where they work: the terminal. It reads, writes, and executes code with full context awareness and persistent memory.",
    },
  ]}
/>

**Omni CLI** is an AI-powered terminal assistant for developers. It provides a TUI (terminal UI), direct CLI mode, and HTTP API, all backed by the shared `agent-core` Rust library.

## Interfaces

| Mode | Usage | Description |
|------|-------|-------------|
| **TUI** | `omni` | Interactive terminal UI (default, ratatui + crossterm) |
| **CLI** | `omni agent "prompt"` | One-shot prompt execution |
| **HTTP** | `omni serve` | API server with SSE streaming (axum) |

## Key Features

- **Multi-provider LLM support**: Anthropic, OpenAI, Google, Groq, Mistral (via agent-core)
- **Tool execution**: File read/write/edit, bash execution, web search, code search, glob/grep
- **MCP support**: Connect to Model Context Protocol servers (stdio and HTTP transports)
- **Skills**: Markdown instruction sets at `.omni/skill/*/SKILL.md` (project-local or global)
- **Plugins**: Auto-discovery from `~/.config/omni/plugins/`
- **Persistent memory**: Per-project memory with categories (preferences, project facts, corrections, general)
- **Session management**: Multi-turn conversations with JSON file persistence
- **LSP integration**: Language Server Protocol support for code intelligence
- **Permission system**: Configurable Allow/Deny/Ask per action type
- **Plan mode**: Read-only analysis mode that prevents write operations

## agent-core

The CLI is built on `agent-core`, a shared Rust library that also powers [Beacon](/grid/beacon). It provides:

| Component | Purpose |
|-----------|---------|
| LLM provider trait | Unified interface with streaming support |
| Provider implementations | Anthropic, OpenAI, Google, Groq, Mistral |
| Conversation manager | Multi-turn state with JSON persistence |
| Permission system | Preset modes per action type |
| Plan mode | Read-only analysis |

## Configuration

Config lives at `~/.config/omni/cli/config.toml`:

```toml
[agent]
provider = "anthropic"  # or "synapse" for unified routing
model = "claude-sonnet-4-20250514"
max_tokens = 8192

[api]
host = "0.0.0.0"
port = 7890
```

Using [Synapse](/grid/synapse) as the provider routes all requests through the AI router, giving you unified key management, smart routing, and automatic failover across providers.

## Skills

Skills are markdown files with YAML frontmatter that teach the agent domain-specific knowledge:

```markdown
---
name: my-skill
description: Brief description
---

# Instructions for the agent
```

Discovery locations (priority order):
1. `.omni/skill/*/SKILL.md` (project-local)
2. `~/.config/omni/skill/*/SKILL.md` (global)

## Memory System

The CLI maintains persistent memory per project (identified by git root commit hash):

| Category | Purpose |
|----------|---------|
| Preference | User preferences (coding style, tools) |
| ProjectFact | Project-specific facts (architecture, patterns) |
| Correction | Things the agent got wrong |
| General | General learned information |

Memories can be pinned (always included in context) or searched by query/category.

## Getting Started

### Prerequisites

- [Rust](https://rustup.rs/) (1.86+)
- An API key for at least one LLM provider (e.g. `ANTHROPIC_API_KEY`)

### Install

```bash
git clone https://github.com/omnidotdev/cli-stack.git
cd cli-stack/services/cli
cargo build --release
```

The binary is at `target/release/omni`. Add it to your `PATH` or copy it to a directory in your `PATH`.

### Configure

Create `~/.config/omni/cli/config.toml`:

```toml
[agent]
provider = "anthropic"
model = "claude-sonnet-4-20250514"
max_tokens = 8192
```

Or route through [Synapse](/grid/synapse) for unified provider management:

```toml
[agent]
provider = "synapse"
model = "claude-sonnet-4-20250514"
```

### Run

```bash
# Launch the TUI (default)
omni

# One-shot CLI mode
omni agent "explain this codebase"

# Start the HTTP API server
omni serve
```

See [Providers](/armory/omni-cli/providers) for full provider configuration and [Configuration](/armory/omni-cli/configuration) for all options.
